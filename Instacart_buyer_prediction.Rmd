---
title: "Instacart Buyer Analysis"
author: "Sandeep Anand"
output:
  html_document: default
  html_notebook: default
---

#Description 

In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user's next order. 

- The Sample submission file has the user's next order and the products present in the next order 
- If there are "n" users them the number of rows in the submission file will be "n"
- Not looking for the best model but a strong F statistic or an accurate model

## Getting the Data
```{r Getting the Data, eval=TRUE, warning=FALSE, echo=FALSE, eval=FALSE}
path1 <- "C:/Public/Instacart/"
for (i in list.files(path=path1, pattern = "\\.zip$")){
  unzip(i, overwrite = TRUE)
  }

```

```{r Data2, eval=TRUE, warning=FALSE, echo=FALSE}
path1 <- "C:/Public/Instacart/"
names <- character(0)
for (j in list.files(path = path1, pattern = "\\.csv$")) 
  {
    names<-c(names,unlist(strsplit(j,"\\."))[1])
}

```

```{r Data3, eval=TRUE, warning=FALSE, echo=FALSE}
library(data.table)
filenames<-list.files(path = path1, pattern = "*.csv$", full.names = TRUE)
ldf <- lapply(filenames, fread)
```

```{r assignment, eval=TRUE, warning=FALSE, echo=FALSE}
for(i in 1:(length(names)-1)){
  nam <- paste(names[i], "dt", sep = ".")
  assign(nam, ldf[[i]])
}
```

## Assignments to be used for Data-frame names:
  - "aisles.dt"
  - "departments.dt"
  - "order_products__prior.dt"
  - "order_products__train.dt"
  - "orders.dt" 

```{r Orders.df Proc, eval=TRUE, warning=FALSE, echo=FALSE}
order_train.dt<-orders.dt[orders.dt$eval_set=="train",]
order_prior.dt<-orders.dt[orders.dt$eval_set=="prior",]
order_test.dt<-orders.dt[orders.dt$eval_set=="test",]

head(order_products__prior.dt)

head(order_prior.dt)
```

## Getting the mapping done For Prior set
  - Get the mapping between order_id and the associated products for it for a particular user
  - New DataTables for getting 
    + user-id, Order-id and count of products for each of the order-ids
    + splitting the data-table that corresponds to orders.csv (filtered based on prior) into multiple sub data tables, based on user_id
    + Get the products for each Order for each of the above data tables and put them into a list that will be an extra row
    + Helper functions are present to break the number of userIds(~206k) into 207 chunks of 1000 each to process them faster
    + chks is a List of length 1 which has another list of length 207
    + Never create multiple variables: not be a wise decision to create multiple vars , why not process and get the output in this function itself

```{r Helper Functions, eval=TRUE, warning=FALSE, echo=FALSE}
require(plyr)
chunks <- function(d, n){      
    chunks <- split(d, ceiling(seq_along(d)/n))
    names(chunks) <- NULL
    return(chunks)
}

chunksplyr <- function(d, n) {
  is<-seq(from=1, to=length(d), by=n)
  if (tail(is,1)!=length(d)) {
    is<-c(is, length(d))
  }
  
  chunks<-ddply(head(seq_along(is),-1),
                function (i) {
                  start <- is[i];
                  end <- is[i+1]-1;
                  d[start:end]
                }
                )
  lc <- length(chunks)
  td<-tail(d,1)
  chunks[[lc]] <- c(chunks[[lc]], td)
  return(chunks)
}

```

## Getting some sense with Plots 

 - Get the Order Count based on users
 - Check & Plot the 10% user_ids who have the maximum number of Orders
 - Get a quantile distribution of the orders , for eg, home many users fall under 95% of the total orders
 - 95% of the Users have less than 50 orders , so presumably if a user has already ordered 50 times , very less likely to re-order - Plotted the QQ distribution plot


```{r Plots with Analysis 1, eval=TRUE, warning=FALSE, echo=FALSE, message=FALSE}
numberobs = length(order_prior.dt$order_id)
numberuserids = unique(order_prior.dt$user_id)
order_prior.dt_cols<-names(order_prior.dt)

ordcount<-order_prior.dt[, .N, by=user_id]


colnames(ordcount)[colnames(ordcount)=="N"]<-"order_count"

library(ggplot2)
b<-ggplot(data = ordcount, aes(x=user_id, y=order_count, alpha=0.8))
b+geom_bar(stat = "identity")

library(dplyr)
new_ordcount <- ordcount[order(-order_count), ]
head(new_ordcount)
quantile(new_ordcount$order_count, probs = seq(0,1,0.05))

ggplot(new_ordcount, aes(sample=order_count)) + stat_qq()


```

 - Get the products info for each order - Use Merge concept here to get this done amazingly quick :
    + Used Inner join concept:  https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html
    + Get the Products data table also merged with the Resultant table to get a better understanding of the products 
    + DT[i, j, by] - Take DT, subset rows using i, then calculate j, grouped by by.
    + The order_prior.dt is handled here for evaluations before the predictions
    + Plot the top 20 products only as most 1% is also a large value to plot

```{r Merge Order_prior and order_product, eval=TRUE, warning=FALSE, echo=FALSE}

setkey(order_prior.dt, order_id)
setkey(order_products__prior.dt, order_id)
res.dt<-order_prior.dt[order_products__prior.dt, nomatch=0]
head(res.dt,20)


setkey(res.dt, product_id)
setkey(products.dt, product_id)
res_final.dt<- res.dt[products.dt, nomatch=0]
head(res_final.dt,20)


res.product.name.count<-res_final.dt[, .(order_id, user_id, product_id, product_name)][order(order_id, -user_id)][, .(.N), by= .(product_name)]
colnames(res.product.name.count)[colnames(res.product.name.count) == "N"] <-"pnames_count"
quantile(res.product.name.count[order(-pnames_count)]$pnames_count, probs = seq(0,1,0.01))

# Plot the top 20 products only as most 1% is also a large value to plot
bar <- ggplot(res.product.name.count[order(-pnames_count)][1:20]) +
  geom_bar(mapping = aes(x=product_name, y=pnames_count), stat = "identity")+
  ggtitle(label = "Top 20 Products Ordered by Users")
  
bar+coord_flip()

product_count <- res_final.dt[, .(order_id, user_id, product_id, product_name)][, .(.N), by= .(order_id, user_id)]
colnames(product_count)[colnames(product_count)=="N"]<-"product_count"
head(product_count, 20)

library(dplyr)
res_sort<-product_count[order(user_id, -product_count),]
head(res_sort,20)


# res_sort has the total number product count of each user for each order_id
ggplot(res_sort[user_id %in% c(1,2)], aes(x = order_id, y=product_count, color=user_id)) + 
  geom_bar(stat = "identity")


```

## Habit analysis of each user based on the products ordered sequence
  - Broken the data set to analyze which user is ordering which product more
  - Used tapply and data table methods to find out that we are having the memory issues so had to break down the data-table

```{r Habit Analysis , warning=FALSE, eval=TRUE, message=FALSE, echo=FALSE}
head(res_final.dt,20)

#Select the columns that you want to do habit analysis on
habit1.dt <- res_final.dt[, .(order_id, user_id, order_number, order_dow, order_hour_of_day, product_id, product_name)]
head(habit1.dt)

#Get the particular columns ordered based on order_id, user_id , order_number
habit1.order1 <- habit1.dt[order(user_id, order_id, order_number)]
head(habit1.order1,20)

# Get the new column with the Frequency of occurence of each Product for each user
habit1.order2 <- habit1.order1[, .(user_id, product_id)]
head(habit1.order2)

#### Getting a List of Data Tables which has the count per user , the Product_Ids 
## This will help me to get which products are highly ordered by each user
# Break to avoid memory issues
#----------------------------------------------------------------
setkey(habit1.order2, user_id)

breakdt1<-habit1.order2[user_id %in% c(1:50000)][, .(Aggregate.count = .N), by=.(user_id, product_id)]
breakdt2<-habit1.order2[user_id %in% c(50001:100001)][, .(Aggregate.count = .N), by=.(user_id, product_id)]
breakdt3<-habit1.order2[user_id %in% c(100001:150001)][, .(Aggregate.count = .N), by=.(user_id, product_id)]
breakdt4<-habit1.order2[user_id %in% c(150001:200001)][, .(Aggregate.count = .N), by=.(user_id, product_id)]
breakdt5<-habit1.order2[user_id %in% c(200001:length(numberuserids))][, .(Aggregate.count = .N), by=.(user_id, product_id)]

#listofdt.peruser.prodid<-tapply(habit1.order2$product_id, habit1.order2$user_id, function(x){as.data.table(table(x))})
#----------------------------------------------------------------

head(habit1.order2,20)

nrow(habit1.dt[habit1.dt$user_id==1,])
```


 
 - Not using the below part of the notebook as that is not the right way to approach the handling of the data set

```{r mapping, eval=TRUE, warning=FALSE, echo=FALSE, eval=FALSE}
cat("The total number of userIds:", length(numberuserids))
#Break the data frame order_prior.dt into multiple dts based on user_id

system.time(chks <- chunks(numberuserids,100))
length(chks)
#system.time(chks2 <- chunksplyr(numberuserids,1000))

dt = data.table("user_id"=integer(), "order_id"=integer(), "product_count"=integer())

#x <- chks[[1]][1:100]



idval <- integer()
order <- integer()
pcount <- integer()

system.time(
for (i in 1:2) {
  x <- chks[[i]][1:100]
  lapply(x, function(x){
    for (i in seq_along(x)) {
      tempval <- order_prior.dt[order_prior.dt$user_id == x[i],]$order_id
      lapply(seq_along(tempval), function(j){
        idval[j] <<- x[i]
        order[j] <<- tempval[j]
        pcount[j] <<- length(which(order_products__prior.dt$order_id == tempval[j]))
      })
    }
    temp <- data.table("user_id"=idval, "order_id"=order, "product_count"=pcount)
    dt <<- rbind(dt, temp)
  })
}
)

system.time(
sapply(chks[1:2], function(x)
  {
    lapply(x, function(x){
      for (i in seq_along(x)) {
        tempval <- order_prior.dt[order_prior.dt$user_id == x[i],]$order_id
        lapply(seq_along(tempval), function(j){
          idval[j] <<- x[i]
          order[j] <<- tempval[j]
          pcount[j] <<- length(which(order_products__prior.dt$order_id == tempval[j]))
        })
      }
      temp <- data.table("user_id"=idval, "order_id"=order, "product_count"=pcount)
      dt <<- rbind(dt, temp)
    })
})
)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
